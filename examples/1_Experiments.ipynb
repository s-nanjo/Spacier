{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43f72996-2112-420d-9e22-ba7518c115e2",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This Jupyter Notebook demonstrates the application of various sampling and optimization techniques in the search for polymers with desired properties. We utilize the `spacier` library for conducting Bayesian Optimization (BO) and other sampling methods to efficiently explore the material space.\n",
    "\n",
    "## Setup and Version Check\n",
    "\n",
    "First, we import necessary libraries and check the version of the `spacier` package to ensure compatibility and reproducibility of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d0b3d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from spacier.ml import spacier\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"spacier: \", spacier.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fdc448-8e30-4958-a7d6-c0847b0d7f8e",
   "metadata": {},
   "source": [
    "## Loading Datasets\n",
    "\n",
    "The following function `load_datasets` is defined to load our datasets from disk. This includes the training data (`df_X`), the pool of candidates (`df_pool_X`), the known outcomes (`df`), the outcomes for the pool (`df_pool`), and the oracle or ground truth data (`df_oracle`). This setup is typical in active learning and Bayesian optimization workflows where an initial dataset is iteratively augmented with carefully selected new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d3ddb9-5bad-4412-bc50-e4c0c538fcdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_datasets():\n",
    "    data_path = \"../spacier/data\"\n",
    "    df_X = pd.read_csv(f\"{data_path}/X.csv\")\n",
    "    df_pool_X = pd.read_csv(f\"{data_path}/X_pool.csv\")\n",
    "\n",
    "    df = pd.read_csv(f\"{data_path}/y.csv\")\n",
    "    df_pool = pd.read_csv(f\"{data_path}/y_pool.csv\")\n",
    "    df_oracle = pd.read_csv(f\"{data_path}//y_oracle.csv\")\n",
    "    \n",
    "    return df_X, df_pool_X, df, df_pool, df_oracle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27325497-5afe-422d-9ab6-696e0d8a9dfb",
   "metadata": {},
   "source": [
    "## Updating Datasets Function\n",
    "\n",
    "The `update_datasets` function is designed to streamline the process of integrating new samples into our existing datasets during each iteration of our experiments. This function takes the current datasets (`df_X`, `df_pool_X`, `df`, `df_pool`), the indices of the new samples (`new_indices`), and the list of properties that need to be updated (`properties`) as inputs. It performs the following tasks:\n",
    "\n",
    "- Selects new samples based on `new_indices` and updates the training dataset (`df_X`) and its corresponding properties (`df`).\n",
    "- Removes these samples from the pool datasets (`df_pool_X`, `df_pool`) to avoid re-selection.\n",
    "- Updates the properties of the selected samples in the training dataset using the oracle (`df_oracle`) to ensure accurate and updated property values.\n",
    "\n",
    "This function returns the updated datasets, ready for the next iteration or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad805e6a-fed3-4b40-b35f-effcf4070c9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_datasets(df_X, df_pool_X, df, df_pool, df_oracle, new_indices, properties):\n",
    "    selected_polymer = df_pool.iloc[new_indices][\"monomer_ID\"]\n",
    "    \n",
    "    # Update the datasets\n",
    "    df_X = pd.concat([df_X, df_pool_X.iloc[new_indices]]).reset_index(drop=True)\n",
    "    df_pool_X = df_pool_X.drop(new_indices).reset_index(drop=True)\n",
    "\n",
    "    df = pd.concat([df, df_pool.iloc[new_indices]]).reset_index(drop=True)\n",
    "    df_pool = df_pool.drop(new_indices).reset_index(drop=True) \n",
    "    \n",
    "    # Update properties from oracle\n",
    "    for prop in properties:\n",
    "        for tmp_ID in selected_polymer:\n",
    "            df.loc[df['monomer_ID'] == tmp_ID, [prop]] = df_oracle[df_oracle[\"monomer_ID\"] == tmp_ID][prop].values[0]\n",
    "    \n",
    "    return df_X, df_pool_X, df, df_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d30819",
   "metadata": {},
   "source": [
    "# Experiment 1: Searching for High Thermal Conductivity Polymers\n",
    "\n",
    "In this experiment, we aim to discover polymers with high thermal conductivity. We employ the Expected Improvement (EI) method for sampling from our candidate pool, comparing its performance to random sampling. EI is a popular acquisition function in Bayesian optimization that balances exploration and exploitation by prioritizing points with a higher expected improvement over the current best observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5779977e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Experiment 1: EI\n",
    "df_X, df_pool_X, df, df_pool, df_oracle = load_datasets()\n",
    "highest_value_ei = [0.19501973]\n",
    "\n",
    "for num in range(20):\n",
    "    new_index = spacier.BO(df_X, df_pool_X, df, df_pool, \"sklearn_GP\", [\"thermal_conductivity\"]).EI(10)\n",
    "    df_X, df_pool_X, df, df_pool = update_datasets(df_X, df_pool_X, df, df_pool, df_oracle, new_index, ['thermal_conductivity'])\n",
    "    highest_value_ei.append(df[\"thermal_conductivity\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f4742b-f884-4262-8b96-8c83e45c3223",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Experiment 1: Random\n",
    "df_X, df_pool_X, df, df_pool, df_oracle = load_datasets()\n",
    "highest_value_random = [0.19501973]\n",
    "\n",
    "for num in range(20):\n",
    "    new_index = spacier.Random(df_X, df_pool_X, df, df_pool).sample(10)\n",
    "    df_X, df_pool_X, df, df_pool = update_datasets(df_X, df_pool_X, df, df_pool, df_oracle, new_index, ['thermal_conductivity'])\n",
    "    highest_value_random.append(df[\"thermal_conductivity\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3f8aa1",
   "metadata": {},
   "source": [
    "### Experiment 1 Plot Explanation\n",
    "\n",
    "The plot below showcases the progress of finding polymers with high thermal conductivity over multiple cycles of sampling. It compares the effectiveness of the Expected Improvement (EI) method against a random sampling strategy. The y-axis represents the highest thermal conductivity found up to each cycle, and the x-axis tracks the number of cycles. This visualization helps to illustrate how quickly and effectively each method identifies polymers with superior thermal conductivity properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b80e74f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 3))\n",
    "plt.xlim(0, 20)\n",
    "plt.ylim(0, 0.7)\n",
    "plt.xlabel(\"cycle\")\n",
    "plt.ylabel(r\"(Max) thermal conductivity [W/(m$\\cdot$K)]\")\n",
    "plt.plot(np.arange(0, 21), highest_value_random, label=\"random\", color=\"grey\", lw=2)\n",
    "plt.plot(np.arange(0, 21), highest_value_ei, label=\"EI\", color=\"k\", lw=2)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2693ac0",
   "metadata": {},
   "source": [
    "# Experiment 2: Searching for Polymers with Specific Properties\n",
    "\n",
    "This experiment focuses on finding polymers that fit within a specified range of properties, such as specific heat capacity (`Cp`), refractive index, and density. The Probability of Improvement (PI) method is used, which selects new samples based on the probability of those samples improving over the best current sample within the desired property ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94956d4a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Experiment 2: PI\n",
    "df_X, df_pool_X, df, df_pool, df_oracle = load_datasets()\n",
    "hit_pi = [0]\n",
    "\n",
    "properties_to_update = [\"Cp\", \"refractive_index\", \"density\"]\n",
    "\n",
    "for num in range(20):\n",
    "    new_index = spacier.BO(df_X, df_pool_X, df, df_pool, \"sklearn_GP\", properties_to_update).PI([[3000, 4000],[1.6, 1.7], [1, 1.1]], 10)\n",
    "    df_X, df_pool_X, df, df_pool = update_datasets(df_X, df_pool_X, df, df_pool, df_oracle, new_index, properties_to_update)\n",
    "    hit_pi.append(len(df.query(\"3000 < Cp < 4000 and 1.6 < refractive_index < 1.7 and 1 < density < 1.1\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf1e57f-97c0-4130-8ec4-fcfba5eeac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: Random\n",
    "df_X, df_pool_X, df, df_pool, df_oracle = load_datasets()\n",
    "hit_random = [0]\n",
    "\n",
    "for num in range(20):\n",
    "    new_index = spacier.Random(df_X, df_pool_X, df, df_pool).sample(10)\n",
    "    df_X, df_pool_X, df, df_pool = update_datasets(df_X, df_pool_X, df, df_pool, df_oracle, new_index, properties_to_update)\n",
    "    hit_random.append(len(df.query(\"3000 < Cp < 4000 and 1.6 < refractive_index < 1.7 and 1 < density < 1.1\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba960ff",
   "metadata": {},
   "source": [
    "### Experiment 2 Plot Explanation\n",
    "\n",
    "This plot demonstrates the efficiency of the Probability of Improvement (PI) method compared to random sampling in identifying polymers within a specified range of properties (specific heat capacity, refractive index, and density). The plot tracks the cumulative number of polymers found that meet the desired criteria over successive sampling cycles. The goal is to highlight the method's ability to zone in on the target property space more efficiently than random chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bafd99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 3))\n",
    "plt.xlim(0, 20)\n",
    "plt.ylim(0, 30)\n",
    "plt.xlabel(\"cycle\")\n",
    "plt.ylabel(\"Number of data in the target area\")\n",
    "plt.plot(np.arange(0, 21), hit_random, label=\"random\", color=\"grey\", lw=2)\n",
    "plt.plot(np.arange(0, 21), hit_pi, label=\"PI\", color=\"k\", lw=2)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf98304",
   "metadata": {},
   "source": [
    "# Experiment 3: Pareto Front Exploration\n",
    "\n",
    "In our third experiment, we explore the Pareto front of polymers based on two competing properties: heat capacity (`Cp`) and refractive index. The Expected Hypervolume Improvement (EHVI) method is a multi-objective optimization technique used here to select new samples that are expected to contribute most to the expansion of the Pareto front. We compare its efficacy against random sampling in identifying true Pareto-optimal solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2509d012",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identifying the Pareto Front\n",
    "df_X, df_pool_X, df, df_pool, df_oracle = load_datasets()\n",
    "PF = spacier.PF_max(df_oracle[\"Cp\"].values, df_oracle[\"refractive_index\"].values)\n",
    "PF_idx = []\n",
    "for num in range(len(PF)):\n",
    "    prop1 = PF[num][0]\n",
    "    prop2 = PF[num][1]\n",
    "    tmp = (df_oracle[\"Cp\"]-prop1).abs() + (df_oracle[\"refractive_index\"]-prop2).abs() \n",
    "    index = df_oracle.index[tmp.argsort()][0].tolist()\n",
    "    PF_idx.append(df_oracle.iloc[index][\"monomer_ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab31232-d33f-4755-bd06-7f7ad09e4582",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Experiment 3: EHVI\n",
    "df_X, df_pool_X, df, df_pool, df_oracle = load_datasets()\n",
    "hit_ehvi = [0]\n",
    "\n",
    "properties_to_update = [\"Cp\", \"refractive_index\"]\n",
    "\n",
    "for num in range(20):\n",
    "    new_index = spacier.BO(df_X, df_pool_X, df, df_pool, \"sklearn_GP\", properties_to_update, standardization=True).EHVI(10)\n",
    "    df_X, df_pool_X, df, df_pool = update_datasets(df_X, df_pool_X, df, df_pool, df_oracle, new_index, properties_to_update)\n",
    "    hit_ehvi.append(len(set(list(df[\"monomer_ID\"])) & set(PF_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4fcea0-620d-47a3-a62e-7e55c57336d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: Random\n",
    "df_X, df_pool_X, df, df_pool, df_oracle = load_datasets()\n",
    "hit_random = [0]\n",
    "\n",
    "for num in range(20):\n",
    "    new_index = spacier.Random(df_X, df_pool_X, df, df_pool).sample(10)\n",
    "    df_X, df_pool_X, df, df_pool = update_datasets(df_X, df_pool_X, df, df_pool, df_oracle, new_index, properties_to_update)\n",
    "    hit_random.append(len(set(list(df[\"monomer_ID\"])) & set(PF_idx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d41a543",
   "metadata": {},
   "source": [
    "### Experiment 3 Plot Explanation\n",
    "\n",
    "In the final experiment, we explore the Pareto front for polymers based on two competing properties: heat capacity (`Cp`) and refractive index. The plot compares the Expected Hypervolume Improvement (EHVI) method against random sampling in terms of discovering true Pareto-optimal solutions over multiple cycles. The number of Pareto solutions found is plotted against the cycle number, illustrating each method's effectiveness in navigating the trade-offs between these two properties to uncover optimal polymers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dac1805",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 3))\n",
    "plt.xlim(0, 20)\n",
    "plt.ylim(0, 30)\n",
    "plt.xlabel(\"cycle\")\n",
    "plt.ylabel(\"Number of pareto solution\")\n",
    "plt.plot(np.arange(0, 21), hit_random, label=\"random\", color=\"grey\", lw=2)\n",
    "plt.plot(np.arange(0, 21), hit_ehvi, label=\"EHVI\", color=\"k\", lw=2)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cff8cda-fcd0-41a5-9a1f-8bbcdce5f7c0",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This notebook demonstrates the application of various sampling methods in the context of materials science, leveraging the `spacier` library. By comparing methods like EI, PI, and EHVI against random sampling, we showcase the potential of Bayesian Optimization and similar strategies for efficient material discovery and optimization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
